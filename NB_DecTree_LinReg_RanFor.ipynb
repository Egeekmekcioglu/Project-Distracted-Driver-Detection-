{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2389743e-b7f8-4888-92cb-f49c5755721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os      #We write a program that will run on both Windows and GNU/Linux.\n",
    "import glob    #For useful easier file or extansion search\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94bb5bd9-86fc-49fa-af56-ec7b2ef451fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(path, img_height, img_width):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (img_height, img_width))# Every img converting to\n",
    "    return resized                                    # 64x64 pixel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1214fc-df4f-44b3-8550-9541ce0779c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():                    \n",
    "    train_directory =\"imgs/train/\" # Data Directory\n",
    "    X_train = []                   # For Train and Test we assign empty array.\n",
    "    y_train = []\n",
    "    \n",
    "    for j in range(10):            # We'll pull the data sequentially from file.\n",
    "        print(\"Load Folder c{}\".format(j))\n",
    "        path = os.path.join(train_directory, 'c' + str(j), '*.jpg') #c0, c1, c2 ...\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl) #Method is used to get \n",
    "                                          #the base name in specified path\n",
    "            img = resize(fl, 64, 64)   # We convert the images to 64x64 size  \n",
    "            X_train.append(img)  # And finally, we added the empty defined array.\n",
    "            y_train.append(j)  # And then, for y_train set.\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2b836ee-4406-4467-98f7-858353398b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Folder c0\n",
      "Load Folder c1\n",
      "Load Folder c2\n",
      "Load Folder c3\n",
      "Load Folder c4\n",
      "Load Folder c5\n",
      "Load Folder c6\n",
      "Load Folder c7\n",
      "Load Folder c8\n",
      "Load Folder c9\n"
     ]
    }
   ],
   "source": [
    "X_train, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e550dc2-f29a-4443-8a28-1b4a2b8b79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22424, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asarray(X_train) # Convert the input to an array.\n",
    "y = np.asarray(y)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4221d97b-6c74-447a-a896-688c24d7096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (17939, 12288)\n",
      "Train Label shape: (17939,)\n"
     ]
    }
   ],
   "source": [
    "#Gives a new shape to an array without changing its data\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "y_train = y.reshape(-1, 1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Train Label shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1c876-9d42-4fa3-bda9-1c80e314106e",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42949c-accf-4c40-b774-ada57c82a21d",
   "metadata": {},
   "source": [
    "Naive Bayes to be used with GaussianNB\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Bayes’ theorem states the following relationship, given class variable \" y \" and dependent feature vector \"X1\" through \" Xn \". \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fbeb35-64f3-4eff-9b85-6836a4c26e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "Mean Accuracy: 0.539941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "print(clf)\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "knn_results = cross_val_score(clf, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "result = \"Mean Accuracy: %f\" % (knn_results.mean())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc88ab2-5254-4e16-a2cb-1e014bff7c7a",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8c762-8f92-47d3-b5b3-15750b40568c",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b17003d-19e1-433e-a791-39b551b71dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "Mean Accuracy: 0.847038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "print(tree)\n",
    "kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "tree_results = cross_val_score(tree, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "result = \"Mean Accuracy: %f\" % (tree_results.mean())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debea61-0cb7-4f98-95fc-3e10de770249",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcd39d-45fa-4048-bdcd-bf4b7bce18f8",
   "metadata": {},
   "source": [
    "Fully Explained:\n",
    "https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389\n",
    "\n",
    "Or creating and using this model also useful for this site without sci-kit function:\n",
    "https://towardsdatascience.com/logistic-regression-explained-and-implemented-in-python-880955306060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4219c9b9-57b8-40f5-82a3-0907e67ec52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, max_iter=300, tol=0.01)\n",
      "Mean Accuracy: 0.991248\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\"\"\"\n",
    "tolfloat, default=1e-4\n",
    "Tolerance for stopping criteria.\n",
    "\n",
    "Cfloat, default=1.0\n",
    "Inverse of regularization strength; \n",
    "must be a positive float. Like in support vector machines, \n",
    "smaller values specify stronger regularization.\n",
    "\n",
    "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "max_iterint, default=100\n",
    "Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "\"\"\"\n",
    "clf1 = LogisticRegression(C=10, tol=0.01, solver='lbfgs', max_iter=300)\n",
    "print(clf1)\n",
    "\n",
    "scores = cross_val_score(clf1, X_train, y_train, cv=5, scoring='accuracy')\n",
    "result = \"Mean Accuracy: %f\" % (scores.mean())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db826154-eae1-41ed-b575-750e4d1ccef9",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be209d-813c-4b0d-9a96-972902e9ea41",
   "metadata": {},
   "source": [
    "Learns a random forest*, which consists of a chosen number of decision trees. Each of the decision tree models is learned on a different set of rows (records) and a different set of columns (describing attributes), whereby the latter can also be a bit-vector or byte-vector descriptor (e.g. molecular fingerprint). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a02ca575-43d4-4ff1-995a-0f65e05bda7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(random_state=42)\n",
      "Mean Accuracy: 0.990468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier(random_state=42)\n",
    "print(clf2)\n",
    "\n",
    "scores = cross_val_score(clf2, X_train, y_train, cv=5, scoring='accuracy')\n",
    "result = \"Mean Accuracy: %f\" % (scores.mean())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b642ec6-9a85-4692-92d4-b0d43ee7d1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
